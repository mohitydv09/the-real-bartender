{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# env import\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Environment**\n",
    "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
    "#@markdown And it's subclass `PushTImageEnv`.\n",
    "#@markdown\n",
    "#@markdown **Goal**: push the gray T-block into the green area.\n",
    "#@markdown\n",
    "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
    "\n",
    "\n",
    "positive_y_is_up: bool = False\n",
    "\"\"\"Make increasing values of y point upwards.\n",
    "\n",
    "When True::\n",
    "\n",
    "    y\n",
    "    ^\n",
    "    |      . (3, 3)\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    +------ > x\n",
    "\n",
    "When False::\n",
    "\n",
    "    +------ > x\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    |      . (3, 3)\n",
    "    v\n",
    "    y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
    "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
    "    local coordinates.\n",
    "\n",
    "    Note that in case positive_y_is_up is False, this function wont actually do\n",
    "    anything except converting the point to integers.\n",
    "    \"\"\"\n",
    "    if positive_y_is_up:\n",
    "        return round(p[0]), surface.get_height() - round(p[1])\n",
    "    else:\n",
    "        return round(p[0]), round(p[1])\n",
    "\n",
    "\n",
    "def light_color(color: SpaceDebugColor):\n",
    "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
    "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
    "    return color\n",
    "\n",
    "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
    "    def __init__(self, surface: pygame.Surface) -> None:\n",
    "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
    "\n",
    "        Typical usage::\n",
    "\n",
    "        >>> import pymunk\n",
    "        >>> surface = pygame.Surface((10,10))\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        You can control the color of a shape by setting shape.color to the color\n",
    "        you want it drawn in::\n",
    "\n",
    "        >>> c = pymunk.Circle(None, 10)\n",
    "        >>> c.color = pygame.Color(\"pink\")\n",
    "\n",
    "        See pygame_util.demo.py for a full example\n",
    "\n",
    "        Since pygame uses a coordiante system where y points down (in contrast\n",
    "        to many other cases), you either have to make the physics simulation\n",
    "        with Pymunk also behave in that way, or flip everything when you draw.\n",
    "\n",
    "        The easiest is probably to just make the simulation behave the same\n",
    "        way as Pygame does. In that way all coordinates used are in the same\n",
    "        orientation and easy to reason about::\n",
    "\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> space.gravity = (0, -1000)\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        To flip the drawing its possible to set the module property\n",
    "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
    "        the simulation upside down before drawing::\n",
    "\n",
    "        >>> positive_y_is_up = True\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0)\n",
    "        >>> # Body will be position in bottom left corner\n",
    "\n",
    "        :Parameters:\n",
    "                surface : pygame.Surface\n",
    "                    Surface that the objects will be drawn on\n",
    "        \"\"\"\n",
    "        self.surface = surface\n",
    "        super(DrawOptions, self).__init__()\n",
    "\n",
    "    def draw_circle(\n",
    "        self,\n",
    "        pos: Vec2d,\n",
    "        angle: float,\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "\n",
    "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
    "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
    "\n",
    "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
    "        p2 = to_pygame(circle_edge, self.surface)\n",
    "        line_r = 2 if radius > 20 else 1\n",
    "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
    "\n",
    "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
    "\n",
    "    def draw_fat_segment(\n",
    "        self,\n",
    "        a: Tuple[float, float],\n",
    "        b: Tuple[float, float],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        r = round(max(1, radius * 2))\n",
    "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
    "        if r > 2:\n",
    "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
    "            if orthog[0] == 0 and orthog[1] == 0:\n",
    "                return\n",
    "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
    "            orthog[0] = round(orthog[0] * scale)\n",
    "            orthog[1] = round(orthog[1] * scale)\n",
    "            points = [\n",
    "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
    "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
    "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
    "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
    "            ]\n",
    "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p1[0]), round(p1[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p2[0]), round(p2[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "\n",
    "    def draw_polygon(\n",
    "        self,\n",
    "        verts: Sequence[Tuple[float, float]],\n",
    "        radius: float,\n",
    "        outline_color: SpaceDebugColor,\n",
    "        fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        ps = [to_pygame(v, self.surface) for v in verts]\n",
    "        ps += [ps[0]]\n",
    "\n",
    "        radius = 2\n",
    "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
    "\n",
    "        if radius > 0:\n",
    "            for i in range(len(verts)):\n",
    "                a = verts[i]\n",
    "                b = verts[(i + 1) % len(verts)]\n",
    "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
    "\n",
    "    def draw_dot(\n",
    "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
    "\n",
    "\n",
    "def pymunk_to_shapely(body, shapes):\n",
    "    geoms = list()\n",
    "    for shape in shapes:\n",
    "        if isinstance(shape, pymunk.shapes.Poly):\n",
    "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
    "            verts += [verts[0]]\n",
    "            geoms.append(sg.Polygon(verts))\n",
    "        else:\n",
    "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
    "    geom = sg.MultiPolygon(geoms)\n",
    "    return geom\n",
    "\n",
    "# env\n",
    "class PushTEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "    reward_range = (0., 1.)\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None, damping=None,\n",
    "            render_action=True,\n",
    "            render_size=96,\n",
    "            reset_to_state=None\n",
    "        ):\n",
    "        self._seed = None\n",
    "        self.seed()\n",
    "        self.window_size = ws = 512  # The size of the PyGame window\n",
    "        self.render_size = render_size\n",
    "        self.sim_hz = 100\n",
    "        # Local controller params.\n",
    "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
    "        self.control_hz = self.metadata['video.frames_per_second']\n",
    "        # legcay set_state for data compatiblity\n",
    "        self.legacy = legacy\n",
    "\n",
    "        # agent_pos, block_pos, block_angle\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
    "            shape=(5,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # positional goal for agent\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0,0], dtype=np.float64),\n",
    "            high=np.array([ws,ws], dtype=np.float64),\n",
    "            shape=(2,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.block_cog = block_cog\n",
    "        self.damping = damping\n",
    "        self.render_action = render_action\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.screen = None\n",
    "\n",
    "        self.space = None\n",
    "        self.teleop = None\n",
    "        self.render_buffer = None\n",
    "        self.latest_action = None\n",
    "        self.reset_to_state = reset_to_state\n",
    "\n",
    "    def reset(self):\n",
    "        seed = self._seed\n",
    "        self._setup()\n",
    "        if self.block_cog is not None:\n",
    "            self.block.center_of_gravity = self.block_cog\n",
    "        if self.damping is not None:\n",
    "            self.space.damping = self.damping\n",
    "\n",
    "        # use legacy RandomState for compatiblity\n",
    "        state = self.reset_to_state\n",
    "        if state is None:\n",
    "            rs = np.random.RandomState(seed=seed)\n",
    "            state = np.array([\n",
    "                rs.randint(50, 450), rs.randint(50, 450),\n",
    "                rs.randint(100, 400), rs.randint(100, 400),\n",
    "                rs.randn() * 2 * np.pi - np.pi\n",
    "                ])\n",
    "        self._set_state(state)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        dt = 1.0 / self.sim_hz\n",
    "        self.n_contact_points = 0\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        if action is not None:\n",
    "            self.latest_action = action\n",
    "            for i in range(n_steps):\n",
    "                # Step PD control.\n",
    "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
    "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
    "                self.agent.velocity += acceleration * dt\n",
    "\n",
    "                # Step physics.\n",
    "                self.space.step(dt)\n",
    "\n",
    "        # compute reward\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
    "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
    "\n",
    "        intersection_area = goal_geom.intersection(block_geom).area\n",
    "        goal_area = goal_geom.area\n",
    "        coverage = intersection_area / goal_area\n",
    "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
    "        done = coverage > self.success_threshold\n",
    "        terminated = done\n",
    "        truncated = done\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self._render_frame(mode)\n",
    "\n",
    "    def teleop_agent(self):\n",
    "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
    "        def act(obs):\n",
    "            act = None\n",
    "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
    "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
    "                self.teleop = True\n",
    "                act = mouse_position\n",
    "            return act\n",
    "        return TeleopAgent(act)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.array(\n",
    "            tuple(self.agent.position) \\\n",
    "            + tuple(self.block.position) \\\n",
    "            + (self.block.angle % (2 * np.pi),))\n",
    "        return obs\n",
    "\n",
    "    def _get_goal_pose_body(self, pose):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        # preserving the legacy assignment order for compatibility\n",
    "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
    "        body.position = pose[:2].tolist()\n",
    "        body.angle = pose[2]\n",
    "        return body\n",
    "\n",
    "    def _get_info(self):\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
    "        info = {\n",
    "            'pos_agent': np.array(self.agent.position),\n",
    "            'vel_agent': np.array(self.agent.velocity),\n",
    "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
    "            'goal_pose': self.goal_pose,\n",
    "            'n_contacts': n_contact_points_per_step}\n",
    "        return info\n",
    "\n",
    "    def _render_frame(self, mode):\n",
    "\n",
    "        if self.window is None and mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        self.screen = canvas\n",
    "\n",
    "        draw_options = DrawOptions(canvas)\n",
    "\n",
    "        # Draw goal pose.\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        for shape in self.block.shapes:\n",
    "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
    "            goal_points += [goal_points[0]]\n",
    "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
    "\n",
    "        # Draw agent and block.\n",
    "        self.space.debug_draw(draw_options)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # the clock is aleady ticked during in step for \"human\"\n",
    "\n",
    "\n",
    "        img = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
    "        if self.render_action:\n",
    "            if self.render_action and (self.latest_action is not None):\n",
    "                action = np.array(self.latest_action)\n",
    "                coord = (action / 512 * 96).astype(np.int32)\n",
    "                marker_size = int(8/96*self.render_size)\n",
    "                thickness = int(1/96*self.render_size)\n",
    "                cv2.drawMarker(img, coord,\n",
    "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                    markerSize=marker_size, thickness=thickness)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0,25536)\n",
    "        self._seed = seed\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def _handle_collision(self, arbiter, space, data):\n",
    "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = state.tolist()\n",
    "        pos_agent = state[:2]\n",
    "        pos_block = state[2:4]\n",
    "        rot_block = state[4]\n",
    "        self.agent.position = pos_agent\n",
    "        # setting angle rotates with respect to center of mass\n",
    "        # therefore will modify the geometric position\n",
    "        # if not the same as CoM\n",
    "        # therefore should be modified first.\n",
    "        if self.legacy:\n",
    "            # for compatiblity with legacy data\n",
    "            self.block.position = pos_block\n",
    "            self.block.angle = rot_block\n",
    "        else:\n",
    "            self.block.angle = rot_block\n",
    "            self.block.position = pos_block\n",
    "\n",
    "        # Run physics to take effect\n",
    "        self.space.step(1.0 / self.sim_hz)\n",
    "\n",
    "    def _set_state_local(self, state_local):\n",
    "        agent_pos_local = state_local[:2]\n",
    "        block_pose_local = state_local[2:]\n",
    "        tf_img_obj = st.AffineTransform(\n",
    "            translation=self.goal_pose[:2],\n",
    "            rotation=self.goal_pose[2])\n",
    "        tf_obj_new = st.AffineTransform(\n",
    "            translation=block_pose_local[:2],\n",
    "            rotation=block_pose_local[2]\n",
    "        )\n",
    "        tf_img_new = st.AffineTransform(\n",
    "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
    "        )\n",
    "        agent_pos_new = tf_img_new(agent_pos_local)\n",
    "        new_state = np.array(\n",
    "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
    "                + [tf_img_new.rotation])\n",
    "        self._set_state(new_state)\n",
    "        return new_state\n",
    "\n",
    "    def _setup(self):\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = 0, 0\n",
    "        self.space.damping = 0\n",
    "        self.teleop = False\n",
    "        self.render_buffer = list()\n",
    "\n",
    "        # Add walls.\n",
    "        walls = [\n",
    "            self._add_segment((5, 506), (5, 5), 2),\n",
    "            self._add_segment((5, 5), (506, 5), 2),\n",
    "            self._add_segment((506, 5), (506, 506), 2),\n",
    "            self._add_segment((5, 506), (506, 506), 2)\n",
    "        ]\n",
    "        self.space.add(*walls)\n",
    "\n",
    "        # Add agent, block, and goal zone.\n",
    "        self.agent = self.add_circle((256, 400), 15)\n",
    "        self.block = self.add_tee((256, 300), 0)\n",
    "        self.goal_color = pygame.Color('LightGreen')\n",
    "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
    "\n",
    "        # Add collision handeling\n",
    "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
    "        self.collision_handeler.post_solve = self._handle_collision\n",
    "        self.n_contact_points = 0\n",
    "\n",
    "        self.max_score = 50 * 100\n",
    "        self.success_threshold = 0.95    # 95% coverage.\n",
    "\n",
    "    def _add_segment(self, a, b, radius):\n",
    "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
    "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
    "        return shape\n",
    "\n",
    "    def add_circle(self, position, radius):\n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 1\n",
    "        shape = pymunk.Circle(body, radius)\n",
    "        shape.color = pygame.Color('RoyalBlue')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_box(self, position, height, width):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        body.position = position\n",
    "        shape = pymunk.Poly.create_box(body, (height, width))\n",
    "        shape.color = pygame.Color('LightSlateGray')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
    "        mass = 1\n",
    "        length = 4\n",
    "        vertices1 = [(-length*scale/2, scale),\n",
    "                                 ( length*scale/2, scale),\n",
    "                                 ( length*scale/2, 0),\n",
    "                                 (-length*scale/2, 0)]\n",
    "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        vertices2 = [(-scale/2, scale),\n",
    "                                 (-scale/2, length*scale),\n",
    "                                 ( scale/2, length*scale),\n",
    "                                 ( scale/2, scale)]\n",
    "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
    "        shape1 = pymunk.Poly(body, vertices1)\n",
    "        shape2 = pymunk.Poly(body, vertices2)\n",
    "        shape1.color = pygame.Color(color)\n",
    "        shape2.color = pygame.Color(color)\n",
    "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
    "        body.position = position\n",
    "        body.angle = angle\n",
    "        body.friction = 1\n",
    "        self.space.add(body, shape1, shape2)\n",
    "        return body\n",
    "\n",
    "\n",
    "class PushTImageEnv(PushTEnv):\n",
    "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "\n",
    "    def __init__(self,\n",
    "            legacy=False,\n",
    "            block_cog=None,\n",
    "            damping=None,\n",
    "            render_size=96):\n",
    "        super().__init__(\n",
    "            legacy=legacy,\n",
    "            block_cog=block_cog,\n",
    "            damping=damping,\n",
    "            render_size=render_size,\n",
    "            render_action=False)\n",
    "        ws = self.window_size\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': spaces.Box(\n",
    "                low=0,\n",
    "                high=1,\n",
    "                shape=(3,render_size,render_size),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_pos': spaces.Box(\n",
    "                low=0,\n",
    "                high=ws,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.render_cache = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        img = super()._render_frame(mode='rgb_array')\n",
    "\n",
    "        agent_pos = np.array(self.agent.position)\n",
    "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
    "        obs = {\n",
    "            'image': img_obs,\n",
    "            'agent_pos': agent_pos\n",
    "        }\n",
    "\n",
    "        # draw action\n",
    "        if self.latest_action is not None:\n",
    "            action = np.array(self.latest_action)\n",
    "            coord = (action / 512 * 96).astype(np.int32)\n",
    "            marker_size = int(8/96*self.render_size)\n",
    "            thickness = int(1/96*self.render_size)\n",
    "            cv2.drawMarker(img, coord,\n",
    "                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
    "                markerSize=marker_size, thickness=thickness)\n",
    "        self.render_cache = img\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode):\n",
    "        assert mode == 'rgb_array'\n",
    "\n",
    "        if self.render_cache is None:\n",
    "            self._get_obs()\n",
    "\n",
    "        return self.render_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs['image'].shape: (3, 96, 96) float32, [0,1]\n",
      "obs['agent_pos'].shape: (2,) float32, [0,512]\n",
      "action.shape:  (2,) float32, [0,512]\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Env Demo**\n",
    "#@markdown Standard Gym Env (0.21.0 API)\n",
    "\n",
    "# 0. create env object\n",
    "env = PushTImageEnv()\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
    "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
    "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Dataset**\n",
    "#@markdown\n",
    "#@markdown Defines `PushTImageDataset` and helper functions\n",
    "#@markdown\n",
    "#@markdown The dataset class\n",
    "#@markdown - Load data ((image, agent_pos), action) from a zarr storage\n",
    "#@markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n",
    "#@markdown - Returns\n",
    "#@markdown  - All possible segments with length `pred_horizon`\n",
    "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
    "#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
    "#@markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n",
    "#@markdown  - key `action`: shape (pred_horizon, 2)\n",
    "\n",
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "# dataset\n",
    "class PushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int):\n",
    "\n",
    "        # read from zarr dataset\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "\n",
    "        # float32, [0,1], (N,96,96,3)\n",
    "        train_image_data = dataset_root['data']['img'][:]\n",
    "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
    "        # (N,3,96,96)\n",
    "\n",
    "        # (N, D)\n",
    "        train_data = {\n",
    "            # first two dims of state vector are agent (i.e. gripper) locations\n",
    "            'agent_pos': dataset_root['data']['state'][:,:2],\n",
    "            'action': dataset_root['data']['action'][:]\n",
    "        }\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        # images are already normalized\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get nomralized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
    "        return nsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['image'].shape: torch.Size([64, 2, 3, 96, 96])\n",
      "batch['agent_pos'].shape: torch.Size([64, 2, 2])\n",
      "batch['action'].shape torch.Size([64, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Dataset Demo**\n",
    "\n",
    "# download demonstration data from Google Drive\n",
    "dataset_path = \"data/pusht_cchi_v7_replay.zarr.zip\"\n",
    "if not os.path.isfile(dataset_path):\n",
    "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
    "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
    "\n",
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "#|o|o|                             observations: 2\n",
    "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTImageDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['image'].shape)\n",
    "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Vision Encoder**\n",
    "#@markdown\n",
    "#@markdown Defines helper functions:\n",
    "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
    "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
    "\n",
    "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
    "    \"\"\"\n",
    "    name: resnet18, resnet34, resnet50\n",
    "    weights: \"IMAGENET1K_V1\", None\n",
    "    \"\"\"\n",
    "    # Use standard ResNet implementation from torchvision\n",
    "    func = getattr(torchvision.models, name)\n",
    "    resnet = func(weights=weights, **kwargs)\n",
    "\n",
    "    # remove the final fully connected layer\n",
    "    # for resnet18, the output dim should be 512\n",
    "    resnet.fc = torch.nn.Identity()\n",
    "    return resnet\n",
    "\n",
    "\n",
    "def replace_submodules(\n",
    "        root_module: nn.Module,\n",
    "        predicate: Callable[[nn.Module], bool],\n",
    "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replace all submodules selected by the predicate with\n",
    "    the output of func.\n",
    "\n",
    "    predicate: Return true if the module is to be replaced.\n",
    "    func: Return new module to use.\n",
    "    \"\"\"\n",
    "    if predicate(root_module):\n",
    "        return func(root_module)\n",
    "\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    for *parent, k in bn_list:\n",
    "        parent_module = root_module\n",
    "        if len(parent) > 0:\n",
    "            parent_module = root_module.get_submodule('.'.join(parent))\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            src_module = parent_module[int(k)]\n",
    "        else:\n",
    "            src_module = getattr(parent_module, k)\n",
    "        tgt_module = func(src_module)\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            parent_module[int(k)] = tgt_module\n",
    "        else:\n",
    "            setattr(parent_module, k, tgt_module)\n",
    "    # verify that all modules are replaced\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    assert len(bn_list) == 0\n",
    "    return root_module\n",
    "\n",
    "def replace_bn_with_gn(\n",
    "    root_module: nn.Module,\n",
    "    features_per_group: int=16) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Relace all BatchNorm layers with GroupNorm.\n",
    "    \"\"\"\n",
    "    replace_submodules(\n",
    "        root_module=root_module,\n",
    "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
    "        func=lambda x: nn.GroupNorm(\n",
    "            num_groups=x.num_features//features_per_group,\n",
    "            num_channels=x.num_features)\n",
    "    )\n",
    "    return root_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.994727e+07\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Network Demo**\n",
    "\n",
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "# ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "# agent_pos is 2 dimensional\n",
    "lowdim_obs_dim = 2\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "\n",
    "# demo\n",
    "with torch.no_grad():\n",
    "    # example inputs\n",
    "    image = torch.zeros((1, obs_horizon,3,96,96))\n",
    "    agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "    # vision encoder\n",
    "    image_features = nets['vision_encoder'](\n",
    "        image.flatten(end_dim=1))\n",
    "    # (2,512)\n",
    "    image_features = image_features.reshape(*image.shape[:2],-1)\n",
    "    # (1,2,512)\n",
    "    obs = torch.cat([image_features, agent_pos],dim=-1)\n",
    "    # (1,2,514)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "    diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "    # the noise prediction network\n",
    "    # takes noisy action, diffusion iteration and observation as input\n",
    "    # predicts the noise added to action\n",
    "    noise = nets['noise_pred_net'](\n",
    "        sample=noised_action,\n",
    "        timestep=diffusion_iter,\n",
    "        global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "    # illustration of removing noise\n",
    "    # the actual noise removal is performed by NoiseScheduler\n",
    "    # and is dependent on the diffusion noise schedule\n",
    "    denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Training**\n",
    "#@markdown\n",
    "#@markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n",
    "#@markdown to load pre-trained weights\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=nets.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nimage = nbatch['image'][:,:obs_horizon].to(device)\n",
    "                nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n",
    "                naction = nbatch['action'].to(device)\n",
    "                B = nagent_pos.shape[0]\n",
    "\n",
    "                # encoder vision features\n",
    "                image_features = nets['vision_encoder'](\n",
    "                    nimage.flatten(end_dim=1))\n",
    "                image_features = image_features.reshape(\n",
    "                    *nimage.shape[:2],-1)\n",
    "                # (B,obs_horizon,D)\n",
    "\n",
    "                # concatenate vision feature and low-dim obs\n",
    "                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                obs_cond = obs_features.flatten(start_dim=1)\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(nets.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_nets = nets\n",
    "ema.copy_to(ema_nets.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110788/1648904295.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path, map_location='cuda')\n"
     ]
    }
   ],
   "source": [
    "#@markdown ### **Loading Pretrained Checkpoint**\n",
    "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
    "\n",
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "  ckpt_path = \"data/pusht_vision_100ep.ckpt\"\n",
    "  if not os.path.isfile(ckpt_path):\n",
    "      id = \"1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\"\n",
    "      gdown.download(id=id, output=ckpt_path, quiet=False)\n",
    "\n",
    "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
    "  ema_nets = nets\n",
    "  ema_nets.load_state_dict(state_dict)\n",
    "  print('Pretrained weights loaded.')\n",
    "else:\n",
    "  print(\"Skipped pretrained weight loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8a39be5f204240aca66bcb0acf818d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval PushTImageEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video controls  width=\"256\"  height=\"256\">\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ### **Inference**\n",
    "\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "max_steps = 200\n",
    "env = PushTImageEnv()\n",
    "# use a seed >200 to avoid initial states seen in the training dataset\n",
    "env.seed(100000)\n",
    "\n",
    "# get first observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon number of observations\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        # normalize observation\n",
    "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "        # images are already normalized to [0,1]\n",
    "        nimages = images\n",
    "\n",
    "        # device transfer\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "        # (2,3,96,96)\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "        # (2,2)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # get image features\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            # (2,512)\n",
    "\n",
    "            # concat with low-dim observations\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end,:]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))\n",
    "\n",
    "# visualize\n",
    "from IPython.display import Video\n",
    "vwrite('data/vis.mp4', imgs)\n",
    "Video('data/vis.mp4', embed=True, width=256, height=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
